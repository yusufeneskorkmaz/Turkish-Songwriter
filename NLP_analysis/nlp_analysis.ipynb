{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2066ef-23be-4f76-9cae-dcf4deb2d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6276dc89-5a22-46f2-a3f8-7160354faf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_turkish_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d99397b-72a4-423c-a40e-f93c7aec5c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yusuf\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-cased\", num_labels=2)\n",
    "model.eval()  # Evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe602c05-9d96-4f94-a998-2b25525ef074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4cee46-db85-4231-a5a1-eb678138c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40cf29b7-2f4f-48ff-b1cd-a5004145e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_batch(texts, batch_size=32):\n",
    "    dataset = LyricsDataset(texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of positive sentiment\n",
    "\n",
    "    return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf20d97e-6c52-425e-bddf-35968c8f0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_score'] = predict_sentiment_batch(df['processed_lyrics'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eec7066b-1f40-461e-98fb-0ed36a3a5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import json\n",
    "\n",
    "# Perform TF-IDF analysis\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['processed_lyrics'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0feb4b60-e831-4dd3-ab9f-b479eaada3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_output = lda.fit_transform(tfidf_matrix)\n",
    "topic_names = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "topic_df = pd.DataFrame(lda_output, columns=topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8eaf704c-c39a-4636-9651-d9895d95d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "df_analyzed = pd.concat([df, tfidf_df, topic_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ca513a7-18bc-4474-8506-7bb38360ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sentiment and top topics for each singer\n",
    "singer_analysis = df_analyzed.groupby('singer').agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'Topic_1': 'mean',\n",
    "    'Topic_2': 'mean',\n",
    "    'Topic_3': 'mean',\n",
    "    'Topic_4': 'mean',\n",
    "    'Topic_5': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc5e5461-aea5-4079-abb0-d5ee9c528028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 TF-IDF words for each singer\n",
    "tfidf_columns = tfidf_df.columns\n",
    "for singer in df_analyzed['singer'].unique():\n",
    "    top_words = df_analyzed[df_analyzed['singer'] == singer][tfidf_columns].mean().nlargest(10).index.tolist()\n",
    "    singer_analysis.loc[singer_analysis['singer'] == singer, 'top_words'] = ', '.join(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d71a24c6-9af4-40b3-a201-b1765743afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge singer analysis back to the main dataframe\n",
    "df_analyzed = df_analyzed.merge(singer_analysis, on='singer', suffixes=('', '_avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0297c4a-a792-4e46-aa2c-0a529da7a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate prompts and completions for LLaMA fine-tuning\n",
    "def generate_llama_training_data(row):\n",
    "    prompt = (f\"Şarkıcı: {row['singer']}\\n\"\n",
    "              f\"Duygu Skoru: {row['sentiment_score_avg']:.2f}\\n\"\n",
    "              f\"Tema Skorları: {', '.join([f'{topic}: {row[topic + '_avg']:.2f}' for topic in topic_names])}\\n\"\n",
    "              f\"Karakteristik Kelimeler: {row['top_words']}\\n\\n\"\n",
    "              f\"Şarkı Sözü:\")\n",
    "    completion = row['lyrics']\n",
    "    return {'prompt': prompt, 'completion': completion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fae949fd-7352-4005-9748-700a9a623d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create LLaMA training data\n",
    "llama_training_data = df_analyzed.apply(generate_llama_training_data, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65544731-3ee8-48f5-95a8-665a466cd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LLaMA training data\n",
    "with open('llama_fine_tuning_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in llama_training_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c286d22a-80e0-41ff-8527-bb363c064bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full analysis results\n",
    "df_analyzed.to_csv('lyrics_full_analysis_for_llama.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a40723-14fd-414f-99fb-b8c07c5020a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
